{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Embeddings\n",
    "\n",
    " - text does not have a inherit semantic distance between words. (hamming-distance...)\n",
    " - Given the text: \"This is the first document\" and \"Is this the first document?\", how similar are these? \n",
    " - Transform text (Graph) into vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-Of-Words \n",
    " - Idea: Create Vocabulary Array of size x, containing every word\n",
    " - For each document ys: count how often each word occurs\n",
    " - x*y matrix containing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer counts occurences in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectoricer : CountVectorizer = CountVectorizer()\n",
    "bow_embedding = bow_vectoricer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returns the Feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectoricer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_embedding.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_embeddings.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    " - Get Cooccurance of words in the document. \n",
    " - The higher the cooccurance between two words, the more similar the vectors should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    " - We want the embeddings of the words\n",
    " - Thus we need to split the sentences into the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [[word for word in document.split(\" \")] for document in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['This', 'is', 'the', 'first', 'document.'],\n",
       " ['This', 'document', 'is', 'the', 'second', 'document.'],\n",
       " ['And', 'this', 'is', 'the', 'third', 'one.'],\n",
       " ['Is', 'this', 'the', 'first', 'document?']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=tokens, vector_size=9, window=2, min_count=1, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_embedding = word2vec.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7fba6c2ede80>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([word for line in tokens for word in line])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this: [-0.09205794 -0.10498687  0.08124185  0.05633624  0.07508548  0.00847628\n",
      "  0.07056545 -0.0378374  -0.01051557]\n",
      "is: [-0.04181524  0.08200561 -0.01703857 -0.05040681  0.07282279 -0.05400178\n",
      " -0.02017797  0.031962    0.01102082]\n",
      "one.: [-0.01615424 -0.10232265  0.04856641  0.00635378  0.08269591 -0.00903735\n",
      " -0.02931856 -0.09726511 -0.00951822]\n",
      "Is: [-0.08536667 -0.01677566  0.02745128 -0.00986536  0.06150129 -0.03047973\n",
      "  0.02510632  0.06060106  0.09273084]\n",
      "third: [ 0.03140464  0.0599922   0.07838441 -0.06337231  0.02067937  0.06765798\n",
      " -0.05332528 -0.03455647  0.07553378]\n",
      "This: [-0.0032909  -0.08512489  0.10683048  0.0553562   0.10259048 -0.09064353\n",
      "  0.04995332 -0.04596751  0.00916151]\n",
      "document?: [ 0.09443673 -0.04957192  0.05022183 -0.07542852 -0.03941385  0.10444671\n",
      " -0.01754775  0.00355158 -0.04598591]\n",
      "second: [ 0.07102815 -0.09577431  0.04073042  0.05766537  0.06379931  0.08296576\n",
      " -0.06852973  0.0122846   0.06719203]\n",
      "document.: [ 0.06409526 -0.08357375 -0.04373448 -0.08346203 -0.0103338   0.1059791\n",
      " -0.08132407 -0.02593076 -0.02153046]\n",
      "And: [ 0.01812751  0.00211019  0.03859597  0.00241975  0.10687584  0.05622893\n",
      " -0.09908211 -0.07823956  0.01001618]\n",
      "document: [-0.03155611 -0.06859469 -0.00455803 -0.09298832 -0.06222236  0.07893932\n",
      "  0.03725044  0.08028522  0.0755583 ]\n",
      "the: [-0.00595808  0.00262702  0.05670388  0.10010304 -0.10336611 -0.07907566\n",
      "  0.07176525  0.09969987 -0.05572698]\n",
      "first: [ 0.0897493  -0.06589884  0.0005018  -0.05281927 -0.10670611  0.05563659\n",
      " -0.09732873 -0.04879806 -0.00039   ]\n"
     ]
    }
   ],
   "source": [
    "for word in vocab:\n",
    "    if(word in word2vec.wv):\n",
    "        print(f\"{word}: {w2v_embedding[word]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/justinmucke/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tokenizer(corpus, return_tensors=\"pt\", padding=True, truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2023, 2003, 1996, 2034, 6254, 1012,  102,    0],\n",
       "        [ 101, 2023, 6254, 2003, 1996, 2117, 6254, 1012,  102],\n",
       "        [ 101, 1998, 2023, 2003, 1996, 2353, 2028, 1012,  102],\n",
       "        [ 101, 2003, 2023, 1996, 2034, 6254, 1029,  102,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**input)\n",
    "word_embeddings = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding of the first word in the first sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n"
     ]
    }
   ],
   "source": [
    "print(len(word_embeddings[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
